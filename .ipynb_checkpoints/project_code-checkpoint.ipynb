{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded imports\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import keras\n",
    "import joblib\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "print \"loaded imports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadAndCropImage(filename):\n",
    "    img = cv2.imread(filename, 0)\n",
    "    img = img[0:1280, 0:720] # crop to 720p\n",
    "    img = cv2.resize(img, (0,0), fx=0.5, fy=0.5)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ALB\n",
      "(100%)                                      \n",
      "Loading: BET\n",
      "(100%)                                      \n",
      "Loading: DOL\n",
      "(100%)                                      \n",
      "Loading: LAG\n",
      "(100%)                                      \n",
      "Loading: NoF\n",
      "(100%)                                      \n",
      "Loading: OTHER\n",
      "(100%)                                      \n",
      "Loading: SHARK\n",
      "(100%)                                      \n",
      "Loading: YFT\n",
      "(100%)                                      \n"
     ]
    }
   ],
   "source": [
    "def loadHOGTrainingData():\n",
    "    training_data = np.array([])\n",
    "    training_labels = np.array([])\n",
    "    folders = [d for d in listdir(\"./train/\") if not isfile(join(\"./train/\", d)) and d not in [\"cropped\"]]\n",
    "    for category in folders:\n",
    "        percent = 0\n",
    "        currentfile = 0.0\n",
    "        images = [file for file in listdir(\"./train/\"+category)]\n",
    "        np.random.shuffle(images)\n",
    "        images = images[0:20] #limit number of images used\n",
    "        print (\"Loading: \" + category)\n",
    "        for image in images:\n",
    "            percent = currentfile/float(len(images))*100.0;\n",
    "            print(\"\\r(\" + str(percent) + \"%)                  \"),\n",
    "            currentfile += 1\n",
    "            img = loadAndCropImage(\"./train/\" + category + \"/\" + image)\n",
    "            hog = cv2.HOGDescriptor()\n",
    "            h = hog.compute(img)\n",
    "            h = h.astype(np.float64)\n",
    "            np.random.shuffle(h)\n",
    "            h = h[0:30000,:] # trim vector so all are same size\n",
    "            vector_data = h.reshape(1,30000) \n",
    "\n",
    "            if len(training_data) == 0:\n",
    "                training_data = np.append(training_data, vector_data)\n",
    "                training_data = training_data.reshape(1,30000)\n",
    "            else:\n",
    "                training_data   = np.concatenate((training_data, vector_data), axis=0)\n",
    "            training_labels = np.append(training_labels,category)\n",
    "            \n",
    "            if (currentfile == len(images)):\n",
    "                print(\"\\r(100%)                  \")\n",
    "                \n",
    "    print(\"loaded training data\")\n",
    "                \n",
    "    return {'data':training_data, 'labels':training_labels}\n",
    "            \n",
    "HOG_training = loadHOGTrainingData()\n",
    "HOG_training_data = HOG_training[\"data\"]\n",
    "training_labels = HOG_training[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "101\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "print \"start\"\n",
    "pca = PCA(n_components=0.7, whiten=True)\n",
    "HOG_PCA_training_data = pca.fit_transform(HOG_training_data)\n",
    "print(len(pca.explained_variance_ratio_)) \n",
    "print \"finished\"\n",
    "df.to_csv('HOG_PCA_training_data.csv', index=False, header=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fishy_svm.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = HOG_PCA_training_data\n",
    "y = training_labels\n",
    "y = y.reshape(y.shape[0],)\n",
    "\n",
    "# Create and fit the SVM\n",
    "# Fitting should take a few minutes\n",
    "clf = MLPClassifier()\n",
    "clf.fit(X,y)\n",
    "joblib.dump(clf, 'fishy_svm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Prediction:\n",
      "['SHARK']\n",
      "[[ 0.13149226  0.07963286  0.13332991  0.09593823  0.1580684   0.07842699\n",
      "   0.2229462   0.10016516]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fishy_svm.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = joblib.load('fishy_svm.pkl') # uncomment to use generated\n",
    "# fishPredict = cv2.imread('./train/LAG/img_04435.jpg', 0)  # Correct is LAG --> Class 3\n",
    "img = loadAndCropImage('./train/LAG/img_00091.jpg')\n",
    "hog = cv2.HOGDescriptor()\n",
    "h = hog.compute(img)\n",
    "h = h.astype(np.float64)\n",
    "np.random.shuffle(h)\n",
    "h = h[0:30000,:] # trim vector so all are same size\n",
    "vector_data = h.reshape(1,30000) \n",
    "vector_data = pca.transform(vector_data)\n",
    "\n",
    "print(\"Linear SVM Prediction:\")\n",
    "print(clf.predict(vector_data))        # prints highest probability class, only\n",
    "print(clf.predict_proba(vector_data))\n",
    "\n",
    "joblib.dump(clf, 'fishy_svm.pkl')\n",
    "    # to load SVM model, use:  clf = joblib.load('filename.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    }
   ],
   "source": [
    "clf = joblib.load('fishy_svm.pkl')\n",
    "hog = cv2.HOGDescriptor()\n",
    "def getPrediction(filename):\n",
    "    img = loadAndCropImage(filename)\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    h = hog.compute(img)\n",
    "    h = h.astype(np.float64)\n",
    "    np.random.shuffle(h)\n",
    "    h = h[0:30000,:] # trim vector so all are same size\n",
    "    vector_data = h.reshape(1,30000) \n",
    "    vector_data = pca.transform(vector_data)\n",
    "    \n",
    "    return clf.predict_proba(vector_data)\n",
    "\n",
    "def savePredictions():\n",
    "    headers = [\"image\", \"ALB\", \"BET\", \"DOL\", \"LAG\", \"NoF\", \"OTHER\", \"SHARK\", \"YFT\"]\n",
    "    predictions = np.array([])\n",
    "    maxi = 0;\n",
    "    print \"start\"\n",
    "    for file in listdir(\"./test_stg1/\"):\n",
    "        prediction = getPrediction(\"./test_stg1/\"+file)\n",
    "        prediction_s = np.array(prediction, dtype='string')\n",
    "        prediction_s = np.insert(prediction_s, 0, file)\n",
    "        prediction_s = prediction_s.reshape(1,9)\n",
    "        if len(predictions) == 0:\n",
    "            predictions = np.append(predictions, prediction_s)\n",
    "            predictions = predictions.reshape(1,9)\n",
    "        else:\n",
    "            predictions   = np.concatenate((predictions, prediction_s), axis=0)\n",
    "        \n",
    "    print predictions\n",
    "    \n",
    "    df = pd.DataFrame(predictions, columns=headers)\n",
    "    df.to_csv('results.csv', index=False, header=True, sep=',')\n",
    "    print \"saved to results.csv\"\n",
    "    \n",
    "savePredictions()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scorePredictions(predictions, values):\n",
    "    summed = 0\n",
    "    i = 0\n",
    "    for prediction in predictions:\n",
    "        if (idxmax() == value):\n",
    "            \n",
    "    print(\"score \" + score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
