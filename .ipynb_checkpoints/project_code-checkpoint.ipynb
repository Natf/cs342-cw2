{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded imports\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import keras\n",
    "import joblib\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "print \"loaded imports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadAndCropImage(filename):\n",
    "    img = cv2.imread(filename, 0)\n",
    "    img = cv2.resize(img, (640, 360)) \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ALB\n",
      "(100%)                      \n",
      "Loading: BET\n",
      "(100%)                         \n",
      "Loading: DOL\n",
      "(100%)                         \n",
      "Loading: LAG\n",
      "(100%)                             \n",
      "Loading: NoF\n",
      "(100%)                             \n",
      "Loading: OTHER\n",
      "(100%)                             \n",
      "Loading: SHARK\n",
      "(100%)                             \n",
      "Loading: YFT\n",
      "(100%)                                  \n",
      "loaded training data\n"
     ]
    }
   ],
   "source": [
    "def loadRawTrainingData():\n",
    "    training_data = np.array([])\n",
    "    training_labels = np.array([])\n",
    "    folders = [d for d in listdir(\"./train/\") if not isfile(join(\"./train/\", d)) and d not in [\"cropped\"]]\n",
    "    for category in folders:\n",
    "        percent = 0\n",
    "        currentfile = 0.0\n",
    "        images = [file for file in listdir(\"./train/\"+category)]\n",
    "        np.random.shuffle(images)\n",
    "        images = images[0:20] #limit number of images used\n",
    "        print (\"Loading: \" + category)\n",
    "        for image in images:\n",
    "            percent = currentfile/float(len(images))*100.0;\n",
    "            print(\"\\r(\" + str(percent) + \"%)                  \"),\n",
    "            currentfile += 1\n",
    "            img = loadAndCropImage(\"./train/\" + category + \"/\" + image)\n",
    "            vector_data = img.reshape(1,230400) \n",
    "\n",
    "            if len(training_data) == 0:\n",
    "                training_data = np.append(training_data, vector_data)\n",
    "                training_data = training_data.reshape(1,230400)\n",
    "            else:\n",
    "                training_data   = np.concatenate((training_data, vector_data), axis=0)\n",
    "            training_labels = np.append(training_labels,category)\n",
    "            \n",
    "            if (currentfile == len(images)):\n",
    "                print(\"\\r(100%)                  \")\n",
    "                \n",
    "    print(\"loaded training data\")\n",
    "                \n",
    "    return {'data':training_data, 'labels':training_labels}\n",
    "            \n",
    "RAW_training = loadRawTrainingData()\n",
    "RAW_training_data = RAW_training['data']\n",
    "RAW_training_labels = RAW_training['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = RAW_training_data\n",
    "y = RAW_training_labels\n",
    "y = y.reshape(y.shape[0],)\n",
    "RAW_MLP = MLPClassifier()\n",
    "RAW_MLP.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "[['img_00005.jpg' '0.0' '0.0' ..., '0.0' '0.0' '0.0']\n",
      " ['img_00007.jpg' '0.0' '0.0' ..., '0.0' '0.0' '0.0']\n",
      " ['img_00009.jpg' '0.0' '0.0' ..., '0.0' '0.0' '0.0']\n",
      " ..., \n",
      " ['img_07908.jpg' '0.0' '0.0' ..., '0.0' '0.0' '0.0']\n",
      " ['img_07910.jpg' '0.0' '0.0' ..., '0.0' '0.0' '0.0']\n",
      " ['img_07921.jpg' '0.0' '0.0' ..., '0.0' '0.0' '0.0']]\n",
      "saved to results.csv\n"
     ]
    }
   ],
   "source": [
    "def getPrediction(filename):\n",
    "    img = loadAndCropImage('./train/LAG/img_00091.jpg')\n",
    "    vector_data = img.reshape(1,230400) \n",
    "    \n",
    "    return RAW_MLP.predict_proba(vector_data)\n",
    "\n",
    "def savePredictions():\n",
    "    headers = [\"image\", \"ALB\", \"BET\", \"DOL\", \"LAG\", \"NoF\", \"OTHER\", \"SHARK\", \"YFT\"]\n",
    "    predictions = np.array([])\n",
    "    maxi = 0;\n",
    "    print \"start\"\n",
    "    for file in listdir(\"./test_stg1/\"):\n",
    "        prediction = getPrediction(\"./test_stg1/\"+file)\n",
    "        prediction_s = np.array(prediction, dtype='string')\n",
    "        prediction_s = np.insert(prediction_s, 0, file)\n",
    "        prediction_s = prediction_s.reshape(1,9)\n",
    "        if len(predictions) == 0:\n",
    "            predictions = np.append(predictions, prediction_s)\n",
    "            predictions = predictions.reshape(1,9)\n",
    "        else:\n",
    "            predictions   = np.concatenate((predictions, prediction_s), axis=0)\n",
    "    \n",
    "    df = pd.DataFrame(predictions, columns=headers)\n",
    "    df.to_csv('results.csv', index=False, header=True, sep=',')\n",
    "    print \"saved to results.csv\"\n",
    "    \n",
    "savePredictions()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18125\n"
     ]
    }
   ],
   "source": [
    "score = RAW_MLP.score(RAW_training_data, RAW_training_labels);\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ALB\n",
      "(100%)                                      \n",
      "Loading: BET\n",
      "(100%)                                      \n",
      "Loading: DOL\n",
      "(100%)                                      \n",
      "Loading: LAG\n",
      "(100%)                                      \n",
      "Loading: NoF\n",
      "(100%)                                      \n",
      "Loading: OTHER\n",
      "(100%)                                      \n",
      "Loading: SHARK\n",
      "(100%)                                      \n",
      "Loading: YFT\n",
      "(100%)                                      \n",
      "loaded training data\n"
     ]
    }
   ],
   "source": [
    "def loadHOGTrainingData():\n",
    "    training_data = np.array([])\n",
    "    training_labels = np.array([])\n",
    "    folders = [d for d in listdir(\"./train/\") if not isfile(join(\"./train/\", d)) and d not in [\"cropped\"]]\n",
    "    for category in folders:\n",
    "        percent = 0\n",
    "        currentfile = 0.0\n",
    "        images = [file for file in listdir(\"./train/\"+category)]\n",
    "        np.random.shuffle(images)\n",
    "        images = images[0:20] #limit number of images used\n",
    "        print (\"Loading: \" + category)\n",
    "        for image in images:\n",
    "            percent = currentfile/float(len(images))*100.0;\n",
    "            print(\"\\r(\" + str(percent) + \"%)                  \"),\n",
    "            currentfile += 1\n",
    "            img = loadAndCropImage(\"./train/\" + category + \"/\" + image)\n",
    "            hog = cv2.HOGDescriptor()\n",
    "            h = hog.compute(img)\n",
    "            h = h.astype(np.float64)\n",
    "            np.random.shuffle(h)\n",
    "            h = h[0:30000,:] # trim vector so all are same size\n",
    "            vector_data = h.reshape(1,30000) \n",
    "\n",
    "            if len(training_data) == 0:\n",
    "                training_data = np.append(training_data, vector_data)\n",
    "                training_data = training_data.reshape(1,30000)\n",
    "            else:\n",
    "                training_data   = np.concatenate((training_data, vector_data), axis=0)\n",
    "            training_labels = np.append(training_labels,category)\n",
    "            \n",
    "            if (currentfile == len(images)):\n",
    "                print(\"\\r(100%)                  \")\n",
    "                \n",
    "    print(\"loaded training data\")\n",
    "                \n",
    "    return {'data':training_data, 'labels':training_labels}\n",
    "            \n",
    "HOG_training = loadHOGTrainingData()\n",
    "HOG_training_data = HOG_training[\"data\"]\n",
    "HOG_training_labels = HOG_training[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.7, whiten=True)\n",
    "HOG_PCA_training_data = pca.fit_transform(HOG_training_data)\n",
    "print(len(pca.explained_variance_ratio_)) \n",
    "df = pd.DataFrame(HOG_PCA_training_data)\n",
    "df.to_csv('HOG_PCA_training_data.csv', index=False, header=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HOG_PCA_MLP.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = HOG_PCA_training_data\n",
    "y = HOG_training_labels\n",
    "y = y.reshape(y.shape[0],)\n",
    "HOG_PCA_MLP = MLPClassifier()\n",
    "HOG_PCA_MLP.fit(X,y)\n",
    "joblib.dump(HOG_PCA_MLP, 'HOG_PCA_MLP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "[['img_00005.jpg' '0.101625829947' '0.090168200702' ..., '0.0866519483585'\n",
      "  '0.224467081952' '0.113276597506']\n",
      " ['img_00007.jpg' '0.150154870059' '0.0915591696494' ..., '0.103151863622'\n",
      "  '0.174157797734' '0.106738991248']\n",
      " ['img_00009.jpg' '0.112565931175' '0.101217377506' ..., '0.104064295386'\n",
      "  '0.223307787316' '0.117894210084']\n",
      " ..., \n",
      " ['img_07908.jpg' '0.118601702101' '0.141178431376' ..., '0.0838477643902'\n",
      "  '0.155612804668' '0.111646313732']\n",
      " ['img_07910.jpg' '0.0855785380803' '0.0591888437491' ...,\n",
      "  '0.0395609406577' '0.264990589688' '0.126941001534']\n",
      " ['img_07921.jpg' '0.141956116339' '0.0955614675587' ..., '0.0670661841732'\n",
      "  '0.172235934501' '0.147568995479']]\n",
      "saved to results.csv\n"
     ]
    }
   ],
   "source": [
    "HOG_PCA_MLP = joblib.load('HOG_PCA_MLP.pkl')\n",
    "hog = cv2.HOGDescriptor()\n",
    "def getPrediction(filename):\n",
    "    img = loadAndCropImage(filename)\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    h = hog.compute(img)\n",
    "    h = h.astype(np.float64)\n",
    "    np.random.shuffle(h)\n",
    "    h = h[0:30000,:] # trim vector so all are same size\n",
    "    vector_data = h.reshape(1,30000) \n",
    "    vector_data = pca.transform(vector_data)\n",
    "    \n",
    "    return HOG_PCA_MLP.predict_proba(vector_data)\n",
    "\n",
    "def savePredictions():\n",
    "    headers = [\"image\", \"ALB\", \"BET\", \"DOL\", \"LAG\", \"NoF\", \"OTHER\", \"SHARK\", \"YFT\"]\n",
    "    predictions = np.array([])\n",
    "    maxi = 0;\n",
    "    print \"start\"\n",
    "    for file in listdir(\"./test_stg1/\"):\n",
    "        prediction = getPrediction(\"./test_stg1/\"+file)\n",
    "        prediction_s = np.array(prediction, dtype='string')\n",
    "        prediction_s = np.insert(prediction_s, 0, file)\n",
    "        prediction_s = prediction_s.reshape(1,9)\n",
    "        if len(predictions) == 0:\n",
    "            predictions = np.append(predictions, prediction_s)\n",
    "            predictions = predictions.reshape(1,9)\n",
    "        else:\n",
    "            predictions   = np.concatenate((predictions, prediction_s), axis=0)\n",
    "    \n",
    "    df = pd.DataFrame(predictions, columns=headers)\n",
    "    df.to_csv('results.csv', index=False, header=True, sep=',')\n",
    "    print \"saved to results.csv\"\n",
    "    \n",
    "savePredictions()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99375\n"
     ]
    }
   ],
   "source": [
    "score = HOG_PCA_MLP.score(HOG_PCA_training_data, HOG_training_labels);\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ALB\n",
      "(100%)                         \n",
      "Loading: BET\n",
      "(100%)                                \n",
      "Loading: DOL\n",
      "(100%)                                      \n",
      "Loading: LAG\n",
      "(100%)                                      \n",
      "Loading: NoF\n",
      "(100%)                                      \n",
      "Loading: OTHER\n",
      "(100%)                                      \n",
      "Loading: SHARK\n",
      "(100%)                                      \n",
      "Loading: YFT\n",
      "(100%)                                      \n",
      "loaded training data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HOG_training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e4dfd032a6d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mPCA_GEO_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadPCAGEOTrainingData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mPCA_GEO_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHOG_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mPCA_GEO_training_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHOG_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HOG_training' is not defined"
     ]
    }
   ],
   "source": [
    "def loadPCAGEOTrainingData():\n",
    "    training_data = np.array([])\n",
    "    training_labels = np.array([])\n",
    "    folders = [d for d in listdir(\"./train/\") if not isfile(join(\"./train/\", d)) and d not in [\"cropped\"]]\n",
    "    for category in folders:\n",
    "        percent = 0\n",
    "        currentfile = 0.0\n",
    "        images = [file for file in listdir(\"./train/\"+category)]\n",
    "        np.random.shuffle(images)\n",
    "        images = images[0:20] #limit number of images used\n",
    "        print (\"Loading: \" + category)\n",
    "        for image in images:\n",
    "            percent = currentfile/float(len(images))*100.0;\n",
    "            print(\"\\r(\" + str(percent) + \"%)                  \"),\n",
    "            currentfile += 1\n",
    "            img = cv2.imread(\"./train/\" + category + \"/\" + image, 0)\n",
    "            img2 = img[0:640]\n",
    "            img = cv2.resize(img, (640, 360)) \n",
    "            img2 = cv2.resize(img2, (640, 360)) \n",
    "            vector_data = img.reshape(1,230400) \n",
    "            vector_data2 = img2.reshape(1,230400) \n",
    "\n",
    "            if len(training_data) == 0:\n",
    "                training_data = np.append(training_data, vector_data)\n",
    "                training_data = training_data.reshape(1,230400)\n",
    "            else:\n",
    "                training_data   = np.concatenate((training_data, vector_data), axis=0)\n",
    "            training_labels = np.append(training_labels,category)\n",
    "            \n",
    "            if len(training_data) == 0:\n",
    "                training_data = np.append(training_data, vector_data2)\n",
    "                training_data = training_data.reshape(1,230400)\n",
    "            else:\n",
    "                training_data   = np.concatenate((training_data, vector_data2), axis=0)\n",
    "            training_labels = np.append(training_labels,category)\n",
    "            \n",
    "            if (currentfile == len(images)):\n",
    "                print(\"\\r(100%)                  \")\n",
    "                \n",
    "    print(\"loaded training data\")\n",
    "                \n",
    "    return {'data':training_data, 'labels':training_labels}\n",
    "            \n",
    "PCA_GEO_training = loadPCAGEOTrainingData()\n",
    "PCA_GEO_training_data = HOG_training[\"data\"]\n",
    "PCA_GEO_training_labels = HOG_training[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = PCA_GEO_training_data\n",
    "y = PCA_GEO_training_labels\n",
    "y = y.reshape(y.shape[0],)\n",
    "PCA_GEO_MLP = MLPClassifier()\n",
    "PCA_GEO_MLP.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = PCA_GEO_MLP.score(PCA_GEO_training_data, PCA_GEO_training_labels);\n",
    "print score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
